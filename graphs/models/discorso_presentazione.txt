good morning everyone, today I am here to present my thesis work entitled "overview of the multi-tast mutual learning technique a comparative analysis of different models
for sentiment analysis and topic detection.

start by presenting a short table of contents consisting of: introduction, in which we explain the general problem, datasets, models, where I show the different models used during the work,
and finally conlusions

In the digital era, the surge in social media and communication platforms has generated an overwhelming volume of textual data. Platforms like Twitter, with 467 million users and 175 million daily tweets, contribute significantly. 
This data influx underscores the pivotal role of Natural Language Processing (NLP) in extracting meaningful insights

Natural Language Processing (NLP) involves computational techniques to analyze and represent natural language, serving specific tasks. Topic detection, a key NLP application, automatically extracts relevant themes from large datasets. 
In the era of big data, Opinion Mining and Sentiment Analysis categorize opinions, providing valuable insights into the public mood.

the multitask mutual learning technique is the basis of my work, there are two models each in charge of a single task but the special feature is that the two models are trained at the same time and exchange information and 
predictions with each other, so that they have information that they would not be able to obtain individually. The weights of the decoder in a neural topic model indicate the association probabilities between words and topics, while the
attention signals in an attention-based classification model captures the importance of words/sentences contributing to the overall sentiment classification. Thus, if we could make these two distributions as similar as possible, we can
potentially generate polarity-bearing topics and at the same time achieve higher sentiment classification results with the topical information incorporated.

the aim of this work was to get an overview of how this technique works, trying to understand how the two models exchange information with each other and the behaviour of the technique with respect different datasets. Once this was done,
I tried to see if changing the two models internally would have an impact on performance, perhaps improving it.

the two datasets used were yelp and imdb, the first containing business reviews and the second one movies reviews. as can be seen from the table, the Yelp dataset is much larger than the IMDB, having about forty thousand documents
compared to the IMDB, which has fifteen thousand, the two datasets have differents number of classes the first one have five classes while the second one ten, the two datasets have similar vocabulay size.

Now we start to introduce the different models that i used in the work, The first method is the variational autoencoder used in the topic detection part; the peculiarity of this model is that it introduces regularisation during the training phase to ensure regularity and 
interpretability of the latent space. the model is composed of two fundamental parts: the green part called the encoder that takes the original data and produced a compressed representation of the data with the essential feature and create
a probability distribution on the latent representation, moreover calculate the mean and the standard deviation. The blue part, called decoder, that reconstructs the input from this compressed representation.This process is optimized iteratively
through backpropagation in order to minimize the difference between the distribution of the input and the reconstruct output.

The second method is recurrent neural networks, used instead for the sentiment analysis part. is primarily designed to identify petterns in sequential data. making them well-suited for sentiment analysis where the order of words
in a sentence is crucial. It consists of an input layer, a hidden layer, this also includes a recurrent part so as to take into account the context relating to the previous time step of the sequence, and finally an output layer.
These first two methods are used in the standard model of the multi-task mutual learning technique, as explained in the slide image above.

The third method is the Bidirectional encoder representing transofrmer. developed by Google, it uses the structure of the tranformer and allows the bidirectional context of a word within the text to be examined.
The model uses a pre-training system in which it learns to predict missing words in a sentence, thus gaining a deep understanding of linguistic relations.
in our work we used Bert-base-uncased which is one of the smallest datasets with which to do pre-training, we also used the standard tokenizer provided by the model,after pre-training we passed our data to execute it.

with this picture we wanted to explain how we incorporated bert's model into our multitask mutual learning model. specifically, we used it in the sentiment analysis part instead of the recurrent neural network. the bert model exchanges
with the topic detection part the information regarding the multi-headed attention to establish the distribution of sentiment.

the fourth and final method is the dirichlet variational autoencoder, which is an extension of the traditional variational autoencoder,the difference being that the dirichlet distribution is incorporated to model the latent variable.This
inclusion allows DirVae to acquire a distributed representation of documents.DirVae aims to learn a distributed representation of documents, where each document is associated with a mixture of topics represented by a Dirichlet
distribution.

here again it is shown how it exchanges information with the sentiment analysis part, it works exactly like the traditional VAE exchanges information on the distribution of words in topics 
 
Now we begin to see the results, the first one is the complexity calculated in execution time. We can see that all models have a high execution time despite running them with only 3 epochs. We can see that there is a trade-off 
between the complexity of one iteration and the complexity of the overall computations.The time decrease increasing the batch size because the complexity of the computation reduce significatively dividing the entire dataset in batches.
When we achieve the best parameter the time start to grows because the number of the operations are extremely large and this affect the benefits due to the easiest calculus. the best result is obtained with a batch size of fifty.

the second result is accuracy,that metric evaluate the number of the correct predictions with respect to the number of the total predictions. here one can generally see a fairly low level, this is due to the impossibility of running the
model many times due to its complexity.It can be seen that the larger the dataset, the less the size of the vocabulary impacts on the result.the two modified models achieve more or less the same results, but the model with DirVAE is better.

In this slide, we have the loss in the topic detection part, evaluate by the KLD measure and the negativa log-likelihood, we can see that here the size of the vocabulary has a greater impact. The two modified models,
on the other hand, perform worse, because the loss is respectively sixty four for the method with BERT and 20 for the method with Dirchlet variational autoencoder instead of the standard model that achieve a loss of 3 

here we have the loss for the classification part evaluate with the Cross-entropy, as opposed to the topic detection part here the variations with respect to vocabulary size are very small, the standard model achieve good results,
but better for the Yelp datasets due to the dimensionality of the dataset, and the two modified models achieve more or less the same results compared with it. 

The last metric used is the The Kullback-Leibler Divergence (KLD),that is a measure of the difference between two probability distributions. It quantifies how one distribution diverges from a second one and provides a measure of the
"distance" between the two distributions Here we can see that the results are very goods in both datasets, with the model with dirVAE and recurrent neural network obtaining the best result.

Summarizing the results, we can say that all results are conditioned by the computational power at our disposal, that the size of the vocabulary has less impact in the larger dataset and that the modified models perform very similarly to the standard model.
in conclusion we can say that this technique requires enormous computational power, there is no evidence to say that one model is superior to another and that this technique is very adaptable to different needs as it can easily change the models within it
possible future work are to implement metrics that help to understand even better, such as the NMI measuring topic robustness, and try to apply this technique to different NLP applications.
 