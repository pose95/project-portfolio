#train model (dopo train_data, train_target, length_data, index_range = train_batch[0], train_batch[1], train_batch[2], train_batch[3]):
if cuda is None:
		###### da qui a...non Ã¨ stata cancellata####
                train_data_var_list = [(torch.LongTensor(chunk)) for chunk in train_data]
                train_target_var = (torch.LongTensor(train_target))
                length_var = (torch.LongTensor(length_data))
                x = (torch.from_numpy(train_X_topic[index_range[0]:index_range[-1] + 1]))
                x_indices = (torch.from_numpy(train_indices[index_range[0]:index_range[-1] + 1]))
		###### qui!#########
            else:
	    	data_var_list = [(torch.LongTensor(chunk).cuda(cuda)) for chunk in data]
            	target_var = (torch.LongTensor(target).cuda(cuda))
            	length_var = (torch.LongTensor(length))
            	x = (torch.from_numpy(test_X_topic[original_index[0]:original_index[-1] + 1]).cuda(cuda))
            	x_indices = (torch.from_numpy(test_indices[original_index[0]:original_index[-1] + 1]).cuda(cuda))
            	# index_arrays = test_index_arrays[original_index[0]:original_index[-1]+1]

#train model(dopo 
            if temp_batch_index % 1000 == 0:
                mean, logvar, p_x_given_h, predicted_train_target, word_attention_dict= model(x, x_indices, train_data_var_list, length_var, cuda):
if len(word_attention_dict) != 0:
                    optimizer_attention.zero_grad()
                    loss_attention = loss_function_share(word_attention_dict, model, loss_function_attention, maxmin)
                    loss_attention.backward()
                    nn.utils.clip_grad_norm(model.continuous_parameters(), max_norm=5)
                    optimizer_attention.step()
