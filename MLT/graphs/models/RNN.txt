Recurrent Neural Networks (RNNs) are a type of neural network architecture which is mainly used
to detect patterns in a sequence of data. Such data can be handwriting, genomes, text or numerical
time series which are often produced in industry settings (e.g. stock markets or sensors) [7, 12].
However, they are also applicable to images if these get respectively decomposed into a series of
patches and treated as a sequence [12]. On a higher level, RNNs find applications in Language
Modelling & Generating Text, Speech Recognition, Generating Image Descriptions or Video Tagging.
What differentiates Recurrent Neural Networks from Feedforward Neural Networks also known
as Multi-Layer Perceptrons (MLPs) is how information gets passed through the network. While
Feedforward Networks pass information through the network without cycles, the RNN has cycles and
transmits information back into itself. This enables them to extend the functionality of Feedforward
Networks to also take into account previous inputs X0:t−1 and not only the current input Xt.

We can describe this process of passing information from the previous iteration to the hidden layer
with the mathematical notation proposed in [24]. For that, we denote the hidden state and the input
at time step t respecively as Ht ∈ R
n×h
and Xt ∈ R
n×d where n is number of samples, d is the
number of inputs of each sample and h is the number of hidden units. Further, we use a weight matrix
Wxh ∈ R
d×h
, hidden-state-to-hidden-state matrix Whh ∈ R
h×h
and a bias parameter bh ∈ R
1×h
.
Lastly, all these informations get passed to a activation function φ which is usually a logistic sigmoid
or tanh function to prepair the gradients for usage in backpropagation.

Since Ht recursively includes Ht−1 and this process occurs for every time step the RNN includes
traces of all hidden states that preceded Ht−1 as well as Ht−1 itself.