https://arxiv.org/pdf/1810.04805.pdf

Language model pre-training has been shown to
be effective for improving many natural language
processing tasks

These include sentence-level tasks such as
natural language inference

and paraphrasing

 which aim to predict the relationships between sentences by analyzing them
holistically, as well as token-level tasks such as
named entity recognition and question answering,
where models are required to produce fine-grained
output at the token level

There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The
feature-based approach, such as ELMo uses task-specific architectures that
include the pre-trained representations as additional features. 

The fine-tuning approach, such as
the Generative Pre-trained Transformer  introduces minimal
task-specific parameters, and is trained on the
downstream tasks by simply fine-tuning all pretrained parameters

The major limitation is that standard language models are
unidirectional, and this limits the choice of architectures that can be used during pre-training

BERT: Bidirectional
Encoder Representations from Transformers.
BERT alleviates the previously mentioned unidirectionality constraint by using a “masked language model” (MLM) pre-training objective, inspired by the Cloze task (Taylor, 1953). The
masked language model randomly masks some of
the tokens from the input, and the objective is to
predict the original vocabulary id of the masked word based only on its contex

MLM objective enables the representation to fuse the left
and the right context, which allows us to pretrain a deep bidirectional Transformer.

There are two steps in our
framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled
data over different pre-training tasks. For finetuning, the BERT model is first initialized with
the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the
downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters

A distinctive feature of BERT is its unified architecture across different tasks

BERT’s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in
the tensor2tensor library

INPUT
spiegazione dei nostri input
The first
token of every sequence is always a special classification token ([CLS]). The final hidden state
First, we separate them with a special
token ([SEP])
For a given token, its input representation is
constructed by summing the corresponding token,
segment, and position embeddings.
imaggine embedding

pre-training del nostro modello 
output del nostro modello




