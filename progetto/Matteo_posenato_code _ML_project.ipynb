{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Welcome to the Emojify Challenge!","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##################################################\n# Imports\n##################################################\n\nimport numpy as np\nimport cv2\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport emoji\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn import preprocessing\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import plot_precision_recall_curve\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n##################################################\n# Params\n##################################################\n\nDATA_BASE_FOLDER = '/kaggle/input/emojify-challenge'\n\n\n##################################################\n# Utils\n##################################################\n\ndef label_to_emoji(label):\n    \"\"\"\n    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n    \"\"\"\n    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"##################################################\n# Load dataset\n##################################################\n\ndf_train = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'train.csv'))\ny_train = df_train['class']\ndf_validation = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'validation.csv'))\ny_validation = df_validation['class']\nemoji_dictionary = {\n    '0': '\\u2764\\uFE0F',\n    '1': ':baseball:',\n    '2': ':smile:',\n    '3': ':disappointed:',\n    '4': ':fork_and_knife:'\n}\n\n# See some data examples\nprint('EXAMPLES:\\n####################')\nfor idx in range(10):\n    print(f'{df_train[\"phrase\"][idx]} -> {label_to_emoji(y_train[idx])}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word embeddings\n\nWords can be represented as n-dimentional vectors where the distance between points has a correspondence respect to similarity between word semantics (similar words are closer, while dissimilar ones are distant). This representation is known as word embeddings and here is extrapolated and pre-computed from the [GloVe](https://nlp.stanford.edu/projects/glove/) model. \n\nHere is depicted an example of bi-dimensional word embeddings:\n![word embedding](https://shanelynnwebsite-mid9n9g1q9y8tt.netdna-ssl.com/wp-content/uploads/2018/01/word-vector-space-similar-words.jpg)\n\nIn our case a single word is represented by a vector of length 25.\n\n# Phrase representation\n\nAll the phrases are padded to the phrase of maximum length, in this case `max_len = 10`, and each phrase is represented by the concatenation of his word embeddings (each phrase thus is a 10 * 25 = 250 dimentional vector).","metadata":{}},{"cell_type":"code","source":"# Load phrase representation\nx_train = np.load(\n    os.path.join(DATA_BASE_FOLDER, \n                 'train.npy')).reshape(len(df_train), -1)\nx_validation = np.load(\n    os.path.join(DATA_BASE_FOLDER, \n                 'validation.npy')).reshape(len(df_validation), -1)\nprint(f'Word embedding size: {x_train.shape[-1]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"exploratory data analysis","metadata":{}},{"cell_type":"code","source":"x_train.shape()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"first of all we want see if the dataset are balenced between the classes","metadata":{}},{"cell_type":"code","source":"df_train['class'].plot(kind = 'hist')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"as we see in the graph we work with unbalanced data then for evaluate our model we use precision-recall and mAP because are usually most robust.","metadata":{}},{"cell_type":"markdown","source":"Preprocessing data ","metadata":{}},{"cell_type":"markdown","source":"Standardization our dataset","metadata":{}},{"cell_type":"code","source":"scaler = preprocessing.StandardScaler().fit(x_train)\nx_train_scaled = scaler.transform(x_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\n\nHere you have to implement a model (or more models, for finding the most accurate) for classification.\n\nYou can use the sklearn (or optionally other more advanced frameworks such as pytorch or tensorflow) package that contains a pool of models already implemented that perform classification. (SVMs, NNs, LR, kNN, ...)","metadata":{}},{"cell_type":"markdown","source":"logistic regression\nimplementation:","metadata":{}},{"cell_type":"code","source":"lr_classification = LogisticRegression()\nlr_fit = lr_classification.fit(x_train, y_train)\ny_pred = lr_fit.predict(x_validation)\nprint(y_pred)\nprint(accuracy(y_pred, y_validation))\nprint(classification_report(y_pred, y_validation))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now we want to optimize our logistic regression function","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparams={\n    'C':[0.01, 0.05, 0.1, 0.5, 1],\n    'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n    'fit_intercept' : [True, False],\n    'class_weight': ['balanced', None],\n    'multi_class': ['auto', 'ovr', 'multinomial']\n}\nlr = LogisticRegression()\ngrid_search_lr = GridSearchCV(estimator=lr, param_grid= params)\ngrid_search_lr.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lrc_opt = LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                          fit_intercept=True,\n                                          intercept_scaling=1, l1_ratio=None,\n                                          max_iter=100, multi_class='auto',\n                                          n_jobs=None, penalty='l2',\n                                          random_state=None, solver='lbfgs',\n                                          tol=0.0001, verbose=0,\n                                          warm_start=False)\nlr_fit_opt=lrc_opt.fit(x_train, y_train)\ny_pred_opt = lr_fit_opt.predict(x_validation)\nprint(y_pred_opt)\nprint(accuracy(y_pred_opt, y_validation))\nprint(classification_report(y_pred_opt, y_validation))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#logistic regression cross validation\nfor c in (5, 10, 15):\n    lrc_CV = LogisticRegressionCV(cv=c, penalty='l2', fit_intercept=False, multi_class='ovr', class_weight = 'balanced')\n    lrc_CV.fit(x_train, y_train)\n    y_pred = lrc_CV.predict(x_validation)\n    print(y_pred)\n    print(accuracy(y_pred, y_validation))\n    print(classification_report(y_pred, y_validation))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now that we have tried with the parameters we try to normalize the trainin data and see what happen","metadata":{}},{"cell_type":"code","source":"#apply to our best model\nlrc_opt.fit(x_train_scaled, y_train)\ny_pred_scaled = lrc_opt.predict(x_validation)\nprint(y_pred_scaled)\nprint(accuracy(y_pred_scaled, y_validation))\nprint(classification_report(y_pred_scaled, y_validation))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now we want to evaluate our model:","metadata":{}},{"cell_type":"markdown","source":"confusion matrix","metadata":{}},{"cell_type":"code","source":"plot_confusion_matrix(lr_fit, x_validation, y_validation)\nplot_confusion_matrix(lr_fit_opt, x_validation, y_validation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"precision and recall curve","metadata":{}},{"cell_type":"code","source":"#precision recall curve\ny_pred_prob = lr_classification.predict_proba(x_validation)\ny_validation_bin = label_binarize(y_validation, classes=[0, 1, 2, 3, 4])\nprecision = dict()\nrecall = dict()\nfor i in range(5):\n    precision[i], recall[i], _ = precision_recall_curve(y_validation_bin[:, i], y_pred_prob[:, i])\n    plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(i))\n    \nplt.xlabel(\"recall\")\nplt.ylabel(\"precision\")\nplt.legend(loc=\"best\")\nplt.title(\"precision vs. recall curve\")\nplt.show()\n\ny_pred_prob_opt = lrc_opt.predict_proba(x_validation)\nprecision = dict()\nrecall = dict()\nfor i in range(5):\n    precision[i], recall[i], _ = precision_recall_curve(y_validation_bin[:, i], y_pred_prob_opt[:, i])\n    plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(i))\n    \nplt.xlabel(\"recall\")\nplt.ylabel(\"precision\")\nplt.legend(loc=\"best\")\nplt.title(\"precision vs. recall curve opt\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ROC curve","metadata":{}},{"cell_type":"code","source":"tpr = dict()\nfpr = dict()\nfor i in range(5):\n    fpr[i], tpr[i], _ = roc_curve(y_validation_bin[:, i], y_pred_prob[:, i])\n    plt.plot(fpr[i], tpr[i], lw=2, label='class {}'.format(i))\n    \nplt.xlabel(\"false positive rate\")\nplt.ylabel(\"true positive rate\")\nplt.legend(loc=\"best\")\nplt.title(\"ROC curve\")\nplt.show()\n\ntpr_opt = dict()\nfpr_opt = dict()\nfor i in range(5):\n    fpr_opt[i], tpr_opt[i], _ = roc_curve(y_validation_bin[:, i], y_pred_prob_opt[:, i])\n    plt.plot(fpr[i], tpr[i], lw=2, label='class {}'.format(i))\n    \nplt.xlabel(\"false positive rate\")\nplt.ylabel(\"true positive rate\")\nplt.legend(loc=\"best\")\nplt.title(\"ROC curve opt\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"diagnosis of bias and variance with learning curve","metadata":{}},{"cell_type":"code","source":"train_size, train_score,valid_score =learning_curve(lr_classification, x_train, y_train)\ntrain_mean = np.mean(train_score, axis=1)\ntrain_std = np.std(train_score, axis=1)\ntest_mean = np.mean(valid_score, axis=1)\ntest_std = np.std(valid_score, axis=1)\n\n#plot\nplt.plot(train_size, train_mean, label=\"training score\")\nplt.plot(train_size, test_mean, label = \"validation score\")\nplt.title(\"Learning Curve\")\nplt.xlabel(\"training size\")\nplt.ylabel(\"score\")\nplt.legend(loc=\"best\")\nplt.show()\n\ntrain_size_opt, train_score_opt,valid_score_opt =learning_curve(lrc_opt, x_train, y_train)\ntrain_mean_opt = np.mean(train_score, axis=1)\ntrain_std_opt = np.std(train_score, axis=1)\ntest_mean_opt = np.mean(valid_score, axis=1)\ntest_std_opt = np.std(valid_score, axis=1)\n\n#plot\nplt.plot(train_size_opt, train_mean_opt, label=\"training score\")\nplt.plot(train_size_opt, test_mean_opt, label = \"validation score\")\nplt.title(\"Learning Curve opt\")\nplt.xlabel(\"training size\")\nplt.ylabel(\"score\")\nplt.legend(loc=\"best\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"support vector machine \nimplementation:","metadata":{}},{"cell_type":"code","source":"svm_classifier = SVC(probability=True)\nsvm_fit = svm_classifier.fit(x_train, y_train)\ny_pred = svm_fit.predict(x_validation)\nprint(y_pred)\nprint(accuracy(y_pred, y_validation))\nprint(classification_report(y_pred, y_validation))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"optimization:","metadata":{}},{"cell_type":"code","source":"#now we tried with the other kernel coefficient\nfor k in ('linear', 'poly', 'rbf', 'sigmoid'):\n    for c in (0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 0.8, 1, 10, 100):\n        for cw in (None, 'balanced'):\n            for g in ('scale', 'auto'):\n                svm_classifier = SVC(C=c, kernel=k, gamma=g, class_weight=cw, probability=True)\n                svm_classifier.fit(x_train, y_train)\n                y_pred_param = svm_classifier.predict(x_validation)\n                print(f'k = {k} c = {c} cw ={cw} gamma = {g} accuracy={accuracy(y_pred_param, y_validation)}  ')\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can conclude that our best svm model have kernel='rbf', gamma='auto', c=10, cw=None","metadata":{}},{"cell_type":"code","source":"svmc_opt = SVC(C=10, kernel='rbf', gamma='auto', probability=True)\nsvm_fit_opt = svmc_opt.fit(x_train, y_train)\ny_pred_opt = svm_fit_opt.predict(x_validation)\nprint(y_pred_opt)\nprint(accuracy(y_pred_opt, y_validation))\nprint(classification_report(y_pred_opt, y_validation))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#apply to our best model\nsvmc_opt.fit(x_train_scaled, y_train)\ny_pred_scaled = svmc_opt.predict(x_validation)\nprint(y_pred_scaled)\nprint(accuracy(y_pred_scaled, y_validation))\nprint(classification_report(y_pred_scaled, y_validation))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"evaluation of the model:","metadata":{}},{"cell_type":"markdown","source":"confusion matrix ","metadata":{}},{"cell_type":"code","source":"#confusion matrix\nplot_confusion_matrix(svm_fit, x_validation, y_validation)\nplot_confusion_matrix(svm_fit_opt, x_validation, y_validation)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"precision recall curve","metadata":{}},{"cell_type":"code","source":"#normail svm classifier\n#precision recall curve\ny_pred_prob = svm_classifier.predict_proba(x_validation)\ny_validation_bin = label_binarize(y_validation, classes=[0, 1, 2, 3, 4])\nprecision = dict()\nrecall = dict()\nfor i in range(5):\n    precision[i], recall[i], _ = precision_recall_curve(y_validation_bin[:, i], y_pred_prob[:, i])\n    plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(i))\n    \nplt.xlabel(\"recall\")\nplt.ylabel(\"precision\")\nplt.legend(loc=\"best\")\nplt.title(\"precision vs. recall curve\")\nplt.show()\n\n#optimize svm classifier\ny_pred_prob_opt = svmc_opt.predict_proba(x_validation)\nprecision = dict()\nrecall = dict()\nfor i in range(5):\n    precision[i], recall[i], _ = precision_recall_curve(y_validation_bin[:, i], y_pred_prob_opt[:, i])\n    plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(i))\n    \nplt.xlabel(\"recall\")\nplt.ylabel(\"precision\")\nplt.legend(loc=\"best\")\nplt.title(\"precision vs. recall curve opt\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ROC curve ","metadata":{}},{"cell_type":"code","source":"#normal svm classifier\ntpr = dict()\nfpr = dict()\nfor i in range(5):\n    fpr[i], tpr[i], _ = roc_curve(y_validation_bin[:, i], y_pred_prob[:, i])\n    plt.plot(fpr[i], tpr[i], lw=2, label='class {}'.format(i))\n    \nplt.xlabel(\"false positive rate\")\nplt.ylabel(\"true positive rate\")\nplt.legend(loc=\"best\")\nplt.title(\"ROC curve\")\nplt.show()\n\n#optimize svm classifer\ntpr = dict()\nfpr = dict()\nfor i in range(5):\n    fpr[i], tpr[i], _ = roc_curve(y_validation_bin[:, i], y_pred_prob_opt[:, i])\n    plt.plot(fpr[i], tpr[i], lw=2, label='class {}'.format(i))\n    \nplt.xlabel(\"false positive rate\")\nplt.ylabel(\"true positive rate\")\nplt.legend(loc=\"best\")\nplt.title(\"ROC curve opt\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"learning curve","metadata":{}},{"cell_type":"code","source":"#normal svm classifier\ntrain_size, train_score,valid_score =learning_curve(svm_classifier, x_train, y_train)\ntrain_mean = np.mean(train_score, axis=1)\ntrain_std = np.std(train_score, axis=1)\ntest_mean = np.mean(valid_score, axis=1)\ntest_std = np.std(valid_score, axis=1)\n\n#plot\nplt.plot(train_size, train_mean, label=\"training score\")\nplt.plot(train_size, test_mean, label = \"validation score\")\nplt.title(\"Learning Curve\")\nplt.xlabel(\"training size\")\nplt.ylabel(\"score\")\nplt.legend(loc=\"best\")\nplt.show()\n\n#optimize svm classifier\ntrain_size_opt, train_score_opt,valid_score_opt =learning_curve(svmc_opt, x_train, y_train)\ntrain_mean_opt = np.mean(train_score_opt, axis=1)\ntrain_std_opt = np.std(train_score_opt, axis=1)\ntest_mean_opt = np.mean(valid_score_opt, axis=1)\ntest_std_opt = np.std(valid_score_opt, axis=1)\n\n#plot\nplt.plot(train_size, train_mean, label=\"training score\")\nplt.plot(train_size, test_mean, label = \"validation score\")\nplt.title(\"Learning Curve opt\")\nplt.xlabel(\"training size\")\nplt.ylabel(\"score\")\nplt.legend(loc=\"best\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"knn\nimplementation:","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors= 5)\nknn_fit = knn.fit(x_train, y_train)\ny_pred = knn.predict(x_validation)\nprint(y_pred)\nprint(accuracy(y_pred,y_validation))\nprint(classification_report(y_pred, y_validation))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"optimization:\nwe try all the different paramaeters to find the best combination\n","metadata":{}},{"cell_type":"code","source":"#now we try with different algorithm\nfor a in ('auto', 'ball_tree', 'kd_tree', 'brute'):\n    for w in ('uniform', 'distance'):\n        for nn in (1, 3, 5, 10):\n            knn = KNeighborsClassifier(n_neighbors = nn, weights=w, algorithm=a)\n            knn.fit(x_train, y_train)\n            y_pred_param = knn.predict(x_validation)\n            print(f'a = {a} w = {w} n = {nn} accuracy={accuracy(y_pred_param, y_validation)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"our model with the best parameter is with algorithm = 'auto'(default), n_neighbors = 3, weight = 'distance' ","metadata":{}},{"cell_type":"code","source":"knn_opt = KNeighborsClassifier(n_neighbors= 3, algorithm='auto', weights = 'distance')\nknn_fit_opt=knn_opt.fit(x_train, y_train)\ny_pred_opt = knn_opt.predict(x_validation)\nprint(y_pred_opt)\nprint(accuracy(y_pred_opt,y_validation))\nprint(classification_report(y_pred_opt, y_validation))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we try our best model with the normalization training set","metadata":{}},{"cell_type":"code","source":"knn_opt.fit(x_train_scaled, y_train)\ny_pred_opt_scaled = knn_opt.predict(x_validation)\nprint(y_pred_opt_scaled)\nprint(accuracy(y_pred_opt_scaled,y_validation))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"didn't improve our accuracy performance.","metadata":{}},{"cell_type":"markdown","source":"evaluation:\nconfusion matrix","metadata":{}},{"cell_type":"code","source":"plot_confusion_matrix(knn_fit, x_validation, y_validation)\nplot_confusion_matrix(knn_fit_opt, x_validation, y_validation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"precision and recall curve","metadata":{}},{"cell_type":"code","source":"#normal KNN\ny_pred_prob = knn.predict_proba(x_validation)\ny_validation_bin = label_binarize(y_validation, classes=[0, 1, 2, 3, 4])\nprecision = dict()\nrecall = dict()\nfor i in range(5):\n    precision[i], recall[i], _ = precision_recall_curve(y_validation_bin[:, i], y_pred_prob[:, i])\n    plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(i))\n    \nplt.xlabel(\"recall\")\nplt.ylabel(\"precision\")\nplt.legend(loc=\"best\")\nplt.title(\"precision vs. recall curve\")\nplt.show()\n\n#optimize KNN\ny_pred_prob_opt = knn_opt.predict_proba(x_validation)\nprecision = dict()\nrecall = dict()\nfor i in range(5):\n    precision[i], recall[i], _ = precision_recall_curve(y_validation_bin[:, i], y_pred_prob_opt[:, i])\n    plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(i))\n    \nplt.xlabel(\"recall\")\nplt.ylabel(\"precision\")\nplt.legend(loc=\"best\")\nplt.title(\"precision vs. recall curve optimize\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ROC curve","metadata":{}},{"cell_type":"code","source":"#normal KNN\ntpr = dict()\nfpr = dict()\nfor i in range(5):\n    fpr[i], tpr[i], _ = roc_curve(y_validation_bin[:, i], y_pred_prob[:, i])\n    plt.plot(fpr[i], tpr[i], lw=2, label='class {}'.format(i))\n    \nplt.xlabel(\"false positive rate\")\nplt.ylabel(\"true positive rate\")\nplt.legend(loc=\"best\")\nplt.title(\"ROC curve\")\nplt.show()\n\n#optimize KNN\ntpr = dict()\nfpr = dict()\nfor i in range(5):\n    fpr[i], tpr[i], _ = roc_curve(y_validation_bin[:, i], y_pred_prob_opt[:, i])\n    plt.plot(fpr[i], tpr[i], lw=2, label='class {}'.format(i))\n    \nplt.xlabel(\"false positive rate\")\nplt.ylabel(\"true positive rate\")\nplt.legend(loc=\"best\")\nplt.title(\"ROC curve optimize\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"learning curve","metadata":{}},{"cell_type":"code","source":"#normal KNN\ntrain_size, train_score,valid_score =learning_curve(knn, x_train, y_train)\ntrain_mean = np.mean(train_score, axis=1)\ntrain_std = np.std(train_score, axis=1)\ntest_mean = np.mean(valid_score, axis=1)\ntest_std = np.std(valid_score, axis=1)\n\n#plot\nplt.plot(train_size, train_mean, label=\"training score\")\nplt.plot(train_size, test_mean, label = \"validation score\")\nplt.title(\"Learning Curve\")\nplt.xlabel(\"training size\")\nplt.ylabel(\"score\")\nplt.legend(loc=\"best\")\nplt.show()\n\n#optimize KNN\ntrain_size_opt, train_score_opt,valid_score_opt =learning_curve(knn_opt, x_train, y_train)\ntrain_mean_opt = np.mean(train_score_opt, axis=1)\ntrain_std_opt = np.std(train_score_opt, axis=1)\ntest_mean_opt = np.mean(valid_score_opt, axis=1)\ntest_std_opt = np.std(valid_score_opt, axis=1)\n\n#plot\nplt.plot(train_size_opt, train_mean_opt, label=\"training score\")\nplt.plot(train_size_opt, test_mean_opt, label = \"validation score\")\nplt.title(\"Learning Curve\")\nplt.xlabel(\"training size\")\nplt.ylabel(\"score\")\nplt.legend(loc=\"best\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"##################################################\n# Evaluate the model here\n##################################################\n\n# Use this function to evaluate your model\ndef accuracy(y_pred, y_true):\n    '''\n    input y_pred: ndarray of shape (N,)\n    input y_true: ndarray of shape (N,)\n    '''\n    return (1.0 * (y_pred == y_true)).mean()\n\n# Report the accuracy in the train and validation sets.\n\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Send the submission for the challenge","metadata":{}},{"cell_type":"code","source":"##################################################\n# Save your test prediction in y_test_pred\n##################################################\n\ny_test_pred = None\n\n# Create submission\nsubmission = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'sample_submission.csv'))\nx_test = np.load(os.path.join(DATA_BASE_FOLDER, 'test.npy')).reshape(len(submission), -1)\nif y_test_pred is not None:\n    submission['class'] = y_test_pred\nsubmission.to_csv('my_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}